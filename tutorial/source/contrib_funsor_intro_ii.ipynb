{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# `pyro.contrib.funsor`: a new backend for Pyro (pt. 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import OrderedDict\n",
    "import functools\n",
    "\n",
    "import torch\n",
    "from torch.distributions import constraints\n",
    "\n",
    "import funsor\n",
    "\n",
    "from pyro import set_rng_seed as pyro_set_rng_seed\n",
    "from pyro.ops.indexing import Vindex\n",
    "from pyro.poutine.messenger import Messenger\n",
    "\n",
    "funsor.set_backend(\"torch\")\n",
    "torch.set_default_dtype(torch.float32)\n",
    "pyro_set_rng_seed(101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "In part 1 of this tutorial, we were introduced to the new `pyro.contrib.funsor` backend for Pyro.\n",
    "\n",
    "Here we'll look at how to use the components in `pyro.contrib.funsor` to implement a variable elimination inference algorithm from scratch. As before, we'll use `pyroapi` so that we can write our model with standard Pyro syntax."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyro.contrib.funsor\n",
    "import pyroapi\n",
    "from pyroapi import infer, handlers, ops, optim, pyro\n",
    "from pyroapi import distributions as dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the following model throughout:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model(data):\n",
    "\n",
    "    p = pyro.param(\"probs\", lambda: torch.rand((3, 3)), constraint=constraints.simplex)\n",
    "    locs_mean = pyro.param(\"locs_mean\", lambda: torch.ones((3,)))\n",
    "    locs = pyro.sample(\"locs\", dist.Normal(locs_mean, 1.).to_event(1))\n",
    "    print(\"locs\", locs.shape)\n",
    "\n",
    "    x = 0\n",
    "    for i in pyro.markov(range(len(data))):\n",
    "        x = pyro.sample(\"x{}\".format(i), dist.Categorical(p[x]), infer={\"enumerate\": \"parallel\"})\n",
    "        print(\"x{}\".format(i), x.shape)\n",
    "        pyro.sample(\"y{}\".format(i), dist.Normal(Vindex(locs)[..., x], 1.), obs=data[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Enumerating discrete variables\n",
    "\n",
    "Our first step is to implement an effect handler that performs parallel enumeration of discrete latent variables. We'll do that by constructing a `funsor.Tensor` representing the support of each discrete latent variable and using the new `pyro.to_data` primitive from part 1 to convert it to a `torch.Tensor` with the appropriate shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.contrib.funsor.handlers.named_messenger import NamedMessenger\n",
    "\n",
    "class EnumMessenger(NamedMessenger):\n",
    "    \n",
    "    @pyroapi.pyro_backend(\"contrib.funsor\")  # necessary since we invoke pyro.to_data\n",
    "    def _pyro_sample(self, msg):\n",
    "        if msg[\"done\"] or msg[\"is_observed\"] or msg[\"infer\"].get(\"enumerate\") != \"parallel\":\n",
    "            return\n",
    "\n",
    "        with funsor.interpreter.interpretation(funsor.terms.lazy):\n",
    "            funsor_dist = pyro.to_funsor(msg[\"fn\"], funsor.reals())(value=msg[\"name\"])\n",
    "            funsor_value = funsor_dist.enumerate_support()\n",
    "\n",
    "        msg[\"value\"] = pyro.to_data(funsor_value)\n",
    "        msg[\"done\"] = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Vectorizing a model across multiple samples\n",
    "\n",
    "Next, since our priors over global variables are continuous and cannot be enumerated exactly, we will implement an effect handler that uses a global dimension to draw multiple samples in parallel from the model. To highlight the interoperability of `pyro.contrib.funsor` with most of Pyro's other machinery, our implementation will allocate a new `DimType.GLOBAL` particle dimension using `pyro.to_data` as in `EnumMessenger` above, then use Pyro's `BroadcastMessenger` to actually draw multiple samples.\n",
    "\n",
    "Recall that in part 1 we saw that `DimType.GLOBAL` dimensions must be deallocated manually or they will persist until the final effect handler has exited. This low-level detail is taken care of automatically by the `GlobalNameMessenger` handler provided in `pyro.contrib.funsor` as a base class for any effect handlers that allocate global dimensions. Our vectorization effect handler will inherit from this class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.poutine.broadcast_messenger import BroadcastMessenger\n",
    "from pyro.poutine.indep_messenger import CondIndepStackFrame\n",
    "\n",
    "from pyro.contrib.funsor.handlers.named_messenger import GlobalNamedMessenger\n",
    "from pyro.contrib.funsor.handlers.runtime import DimType\n",
    "\n",
    "class VectorizeMessenger(GlobalNamedMessenger):\n",
    "    \n",
    "    def __init__(self, size, name=\"_PARTICLES\"):\n",
    "        super().__init__()\n",
    "        self.name = name\n",
    "        self.size = size\n",
    "        self._indices = funsor.Tensor(\n",
    "            torch.arange(0, self.size),\n",
    "            OrderedDict([(self.name, funsor.bint(self.size))])\n",
    "        )\n",
    "\n",
    "    @pyroapi.pyro_backend(\"contrib.funsor\")\n",
    "    def __enter__(self):\n",
    "        super().__enter__()  # do this first to take care of global dim recycling\n",
    "        # Here we indicate that the vectorization dimension is a DimType.GLOBAL dimension,\n",
    "        # as opposed to a DimType.VISIBLE dimension that we would use in pyro.plate\n",
    "        self.indices = pyro.to_data(self._indices, dim_type=DimType.GLOBAL)\n",
    "        return self\n",
    "\n",
    "    def _pyro_sample(self, msg):\n",
    "        frame = CondIndepStackFrame(self.name, -self.indices.dim(), self.size, 0)\n",
    "        msg[\"cond_indep_stack\"] = (frame,) + msg[\"cond_indep_stack\"]\n",
    "        BroadcastMessenger._pyro_sample(msg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Computing an ELBO with variable elimination\n",
    "\n",
    "Our final effect handler will build up a lazy Funsor expression for the log of a Monte Carlo estimate of the marginal likelihood, a lower bound on the true log-evidence. We will call `pyro.to_funsor` on both the sample value and the distribution, showing how nearly all inference operations including log-probability density evaluation can be performed on `funsor.Funsor`s directly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LogJointMessenger(Messenger):\n",
    "\n",
    "    def __enter__(self):\n",
    "        self.log_joint = funsor.Number(0.)\n",
    "        return super().__enter__()\n",
    "\n",
    "    @pyroapi.pyro_backend(\"contrib.funsor\")\n",
    "    def _pyro_post_sample(self, msg):\n",
    "        with funsor.interpreter.interpretation(funsor.terms.lazy):\n",
    "            funsor_dist = pyro.to_funsor(msg[\"fn\"], output=funsor.reals())\n",
    "            funsor_value = pyro.to_funsor(msg[\"value\"], output=funsor_dist.inputs[\"value\"])\n",
    "            self.log_joint += funsor_dist(value=funsor_value)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally the actual loss function, which applies our three effect handlers and evaluates the final lazy expression using Funsor's `optimize` interpretation for variable elimination."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "@pyroapi.pyro_backend(\"contrib.funsor\")\n",
    "def log_z(model, *args, size=10):\n",
    "    with LogJointMessenger() as tr, \\\n",
    "            VectorizeMessenger(size=size) as v, \\\n",
    "            EnumMessenger():\n",
    "        model(*args)\n",
    "\n",
    "    with funsor.interpreter.interpretation(funsor.terms.lazy):\n",
    "        prod_vars = frozenset({v.name})\n",
    "        sum_vars = frozenset(tr.log_joint.inputs) - prod_vars\n",
    "        expr = tr.log_joint.reduce(funsor.ops.logaddexp, sum_vars)\n",
    "        expr = expr.reduce(funsor.ops.add, prod_vars)\n",
    "\n",
    "    return pyro.to_data(funsor.optimizer.apply_optimizer(expr))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Putting it all together\n",
    "\n",
    "Finally, with all this machinery implemented, we can perform inference in our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "locs torch.Size([10, 1, 1, 1, 1, 3])\n",
      "x0 torch.Size([3, 1, 1, 1, 1, 1])\n",
      "x1 torch.Size([3, 1, 1, 1, 1, 1, 1])\n",
      "x2 torch.Size([3, 1, 1, 1, 1, 1])\n",
      "x3 torch.Size([3, 1, 1, 1, 1, 1, 1])\n",
      "x4 torch.Size([3, 1, 1, 1, 1, 1])\n",
      "x5 torch.Size([3, 1, 1, 1, 1, 1, 1])\n",
      "x6 torch.Size([3, 1, 1, 1, 1, 1])\n",
      "x7 torch.Size([3, 1, 1, 1, 1, 1, 1])\n",
      "x8 torch.Size([3, 1, 1, 1, 1, 1])\n",
      "x9 torch.Size([3, 1, 1, 1, 1, 1, 1])\n",
      "tensor(-175.6081, grad_fn=<SumBackward0>) (tensor([[ 3.4911,  0.7792, -4.2704],\n",
      "        [ 1.5555,  0.1515, -1.7070],\n",
      "        [ 0.1272,  0.2921, -0.4193]]), tensor([10.8165, -0.1219, -1.6229]))\n"
     ]
    }
   ],
   "source": [
    "data = [torch.tensor(1.)] * 10\n",
    "log_marginal = log_z(model, data)\n",
    "params = [pyro.param(\"probs\").unconstrained(), pyro.param(\"locs_mean\").unconstrained()]\n",
    "print(log_marginal, torch.autograd.grad(log_marginal, params))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
